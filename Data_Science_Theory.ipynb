{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Origins of Data Science\n",
    "\n",
    "## History \n",
    "The term has existed over 30 years and is originally attributed to Prof. Peter Naur from University of Copenhagen in 1974. Those days Data Science mostly referred to the process of data processing methods that acquired sophistication over time. A statistical dimension was provided by Prof. Jeff Wu at the Univeristy of Michigan in 1997 at the Indian Statistical Institute famously titled \"Statistics = Data Science?\". However, Data Science has evovled since then adding more dimensions such as Machine Learning, Applied Mathematics and other Engineering methods. \n",
    "\n",
    "#### Definition\n",
    "Wiki defines Data Science as \"Data Science employs techniques and theories drawn from many fields with the broad areas of mathematics, statistics, operations research, information science and computer science, including signal processing, probability models, machine learning, statistical learning, data mininig, database, data engineering, pattern recognition and learning, visualization, predictive analysis, uncertainty modeling, data warehousing, data compression, computer programming, artificial intelligence, and high performance computing.\" This definition mostly arises from a variety of fields within the academia and industry which have shaped the field. The area of Data Science, is still evolving with many fields from Physics, Mathematics, Statistics and Engineering adding a plethora of methods, all aimed at solving complex problems.\n",
    "\n",
    "### What is Data Science? \n",
    "Data Science refers to solving complex problems by understanding data through exploratory data analysis. This involves many steps, primary amongst which are  Exploratory Data Analysis (EDA), Modeling and Algorithms followed by results and Data Visualization.\n",
    "\n",
    "### Who is a Data Scientist?\n",
    "A Data Scientist is an individual who can use data and statistical ability to discover, explore and interpret the given data and thereby building mathematical models that fits the data and then produce useful results. Harvard Business Review called the job as \"The sexiest job of the 21st Century\". \n",
    "\n",
    "### Technology Stack\n",
    "Data Scientist technology stack varies across different organizations depending on the need. Primarily, Data Scientists use Python, R, various Python libraries such as Pandas and Scikit-Learn. Few Data Scientists could be expected to also work on large data sets that range in 100s of GB or several TBs, though typically Data Engineers and Computer Progammers fill in for roles of scaling these algorithms. \n",
    "\n",
    "### Course Prerequisites\n",
    "Basic knowledge of Python\n",
    "Mathematics - Including Calculus, Linear Algebra, Probability and Statistics.\n",
    "\n",
    "### Instructions\n",
    "One of the key factors known to have sparked off interest in Data Science is the famous \"Netflix Prize\", a competition that provided $1 Million to any winning team or individual who could build the best recommendation engine for movie ratings to predict user ratings for movies, just based on previous ratings. \n",
    "\n",
    "Suppose a user ranks interest in each category from a scale of 0 - 10:\n",
    "\n",
    "User Interest of Movie category\n",
    "Feel Good\tHorror\tComedy\tAdult\tTragedy\n",
    "10.0\t0.0\t5.0\t1.0\t2.0\n",
    "How do we find a movie that has components as shown below that aligns with the user? Let us use a prediction metric to predict the movie match to the user on a scale of 1 to 10.\n",
    " \n",
    "## 1. Exercise\n",
    "\n",
    "Movie Category\n",
    "Feel Good\tHorror\tComedy\tAdult\tTragedy\n",
    "7.0\t2.0\t9.0\t7.0\t2.0\n",
    "This is a sample lesson that illustrates how to work with the console. You can edit code as per instructions, assign results to variables and then click on Run to execute them. \n",
    "\n",
    "Click on Run to detemine what is the match of the movie to the user preference on a scale of 1 to 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie match to the user on a scale of 1 to 10: 8.06225774829855\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "# Prediction of movie interest based on user preferences on a scale to 1-10\n",
    "user_pref = [10.0, 0.0, 5.0, 1.0, 2.0]\n",
    "movie_features = [7.0, 2.0, 9.0, 7.0, 2.0]\n",
    "\n",
    "user_match = distance.euclidean(user_pref, movie_features)\n",
    "print(\"Movie match to the user on a scale of 1 to 10:\", user_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Data Science Workflow\n",
    "\n",
    "## Pipeline\n",
    "A Data Science workflow or a pipeline refers to the standard activities that a Data Scientist performs from acquiring data to communicating the final results. \n",
    "\n",
    "Here are the important steps in the Data Science pipeline:\n",
    "\n",
    "Data Acquisition\n",
    "Exploratory Data Analysis (EDA)\n",
    "Problem Identification\n",
    "Modeling\n",
    "Model Validation and Fine Tuning\n",
    "Communicating Final Results\n",
    "Scaling and Big Data\n",
    "\n",
    "## 1. Data Acquisition\n",
    "Acquiring data is the first step in the pipeline. This involves working with Data Engineers or Infrastructure Engineers to provide data in a structured format such as JSON, csv, or Text. Data Engineers are expected to provide the data in the known format to the Data Scientists. This involves parsing the data and pushing it to a SQL database or a format that is easy to work with. This can involve applying a known schema to the data that is already known or can be inferred from the original data. When original data is in unstructured format, the data needs to be cleaned and relevant data extracted from it. This involves using a regular expression parser or multiple methods of parsing such as using perl and unix scripts, or language of your choice to clean the data.\n",
    "\n",
    "## 2. Exercise\n",
    "\n",
    "#### Instructions\n",
    "* Given the csv file about Women in STEM, read the contents of the file line by line and push it to a list variable,\n",
    "stem_women.\n",
    "* Print out first 5 elements of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/Colaberry/538data/master/college-majors/women-stem.csv\"\n",
    "r = requests.get(url)\n",
    "text = r.iter_lines()\n",
    "stem_women = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Exploratory Data Analysis\n",
    "\n",
    "2. Exploratory Data Analysis (EDA)\n",
    "Exploratory Data Analysis is the second step which involves looking at various statistics and visualizations generated from various dimensions of the dataset. This helps identifying anomalies, errors and identifying other areas of Machine Learning problems. For example, let us say we are looking to identify a dataset that has a column full of social security numbers. Just by computing a list of unique values whose frequency counts are above a threshold number such as 10, could lead us to see spurious numbers such as 000-00-0000. This is clearly wrong as we can see and could lead to problems in applying machine learning techniques. There are other ways by looking at the graphs too which we can identify such spurious data. Hence, this is the crucial step in the Data Science pipeline.\n",
    "\n",
    "## 3. Problem Identification\n",
    "Post EDA, we can identify if the any of the Machine Learning models such as prediction, clustering can be applied to the dataset.\n",
    "\n",
    "## 4. Modeling\n",
    "Modeling refers to applying Machine Learning models the dataset that follow basic principles. We shall study about these models in the future lessons.\n",
    "\n",
    "## 5. Model Validation and Fine Tuning\n",
    "The models we have built so far need to be validated for performance. Later, most often, a parameter tuning is performed which increases the performance of the model on the test dataset. \n",
    "\n",
    "## 6. Communicating Final Results\n",
    "It is important to communicate final results to the business or non-technical audience. Hence, visualization forms an important part of Data Science. We shall learn how to couple great visualizations in your Data Science pipeline to effectively communciate your results.\n",
    "\n",
    "## 7. Scaling and Big Data\n",
    "The models that perform greatly on small datasets might not do so on large datasets due to the variance present in the dataset. Hence, working with big data and scaling up the algorithms is a challenge. The models are initially validated with small datasets before working with big data.\n",
    "\n",
    "## 2. Exercise\n",
    "\n",
    "#### Instructions\n",
    "NBA Win Probabilities\n",
    "The data contains every NBA teamâ€™s chance of winning in every minute across every Game:\n",
    "\n",
    "https://fivethirtyeight.com/features/every-nba-teams-chance-of-winning-in-every-minute-across-every-game/\n",
    "\n",
    "Given these probabilities, find out which team is most likely to win in all games?\n",
    "Assign the team name to the variable, winning_team and print it out. \n",
    "Use the Hint feature to look up the command to print out the answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "winning_team = 'None'\n",
    "\n",
    "nba_data = pd.read_csv(\"https://raw.githubusercontent.com/colaberry/538data/master/nba-winprobs/nba.tsv\", sep = '\\t')\n",
    "winning_team_row = nba_data[nba_data['48'] == nba_data['48'].max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
